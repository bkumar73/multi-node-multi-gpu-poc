kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: vllm-llama3-8b
  namespace: demo-multi-node-multi-gpu
  labels:
    app: isvc.vllm-llama3-8b-predictor
    component: predictor
    isvc.generation: "1"
    serving.kserve.io/inferenceservice: vllm-llama3-8b
  annotations:
    openshift.io/host.generated: "true"
spec:
  to:
    kind: Service
    name: vllm-llama3-8b-predictor
    weight: 100
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
